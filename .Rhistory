slice(index) -> happiness_train
set.seed(777) #For reproducibility
n = nrow(train)
index <- sample(1:n,300)
train %>%
slice(index) -> happiness_train
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(forecast)
library(olsrr)
library(glmnet)
library(pROC)
library(caret)
library(janitor)
library(e1071)
library(rpart)
library(rpart.plot)
library(DataExplorer)
train <- read.csv("classification_train.csv")
test <- read.csv("classification_test.csv")
train <- train %>%
mutate(perfectMentalHealth = as.factor(perfectMentalHealth))
test <- test %>%
mutate_if(sapply(test, is.character), as.factor)
library(fastDummies)
categorical_var = names(train)[sapply(train, is.character)]
categorical_var = categorical_var[categorical_var != 'perfectMentalHealth']
train <- fastDummies::dummy_cols(train, categorical_var, remove_selected_columns = TRUE)
set.seed(1000)
train_index <- createDataPartition(train$perfectMentalHealth, p = 0.7, list = FALSE)
train_df <- train[train_index, ]
test_df <- train[-train_index, ]
library(MASS)
logit_lr <- polr(perfectMentalHealth ~.,
data = train_df,
Hess = TRUE)
# Predict the class labels for the test set
predicted_classes <- predict(logit_lr, newdata = test_df, type = "class")
View(test)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
train <- read.csv("regression_train.csv")
test <- read.csv("regression_test.csv")
View(train)
setwd("~/Documents/ds_portfolio/ds_portfolio_github-repo/student-performance/student-performance")
library(tidyverse)
library(glmnet)
student_df = read_csv("StudentsPerformance.csv")
head(student_df)
set.seed(32022)
n=nrow(student_df)
index = sample(1:n,100)
student_df %>%
slice(index) -> studentTrain
student_df %>%
slice(-index) -> studentValid
set.seed(32022)
n=nrow(student_df)
index = sample(1:n,100)
student_df %>%
slice(index) -> studentTrain
student_df %>%
slice(-index) -> studentValid
summary(model1)
model1 = glm(math_score ~ reading_score + writing_score ,data = studdentTrain)
model1 = glm(math_score ~ reading_score + writing_score ,data = studentTrain)
model1 = glm(math score ~ reading score + writing score ,data = studentTrain)
model1 = glm(`math score` ~ `reading score` + `writing score` ,data = studentTrain)
summary(model1)
print(R1)
R1 = 1 - model1$deviance/model1$null.deviance
R1 = 1 - model1$deviance/model1$null.deviance
print(R1)
setwd("~/Documents/ds_portfolio/ds_portfolio_github-repo/student-performance/student-performance")
student_df <- dummy_cols(student_df)
library(fastDummies)
student_df <- dummy_cols(student_df)
View(student_df)
View(student_df)
head(student_df)
library(fastDummies)
student_df <- dummy_cols(
student_df,
remove_first_dummy = TRUE
)
head(student_df)
student_df
library(fastDummies)
# Create dummy variables
student_df <- dummy_cols(student_df, remove_first_dummy = TRUE)
# Explicitly drop unwanted reference groups (if needed)
student_df <- student_df %>%
select(-`race/ethnicity_group A`)  # Example: Drop 'group A' as the reference
student_df
View(student_df)
library(fastDummies)
# Create dummy variables
student_df <- dummy_cols(student_df, remove_first_dummy = TRUE)
# Explicitly drop unwanted reference groups (if needed)
student_df <- student_df %>%
select(-`race/ethnicity_group A`, -`gender`)  # Example: Drop 'group A' as the reference
# Explicitly drop unwanted reference groups (if needed)
student_df <- student_df %>%
select(-`race/ethnicity_group A`, -`gender_female`)  # Example: Drop 'group A' as the reference
# Explicitly drop unwanted reference groups (if needed)
student_df <- student_df %>%
select(-`race/ethnicity_group A`, `gender_female`)  # Example: Drop 'group A' as the reference
# Explicitly drop unwanted reference groups (if needed)
student_df <- student_df %>%
select(c(-`race/ethnicity_group A`, `gender_female`))  # Example: Drop 'group A' as the reference
library(fastDummies)
student_df <- dummy_cols(student_df, remove_first_dummy = TRUE)
student_df <- student_df %>%
select(-`race/ethnicity_group A`, -`gender_female`, -`lunch_standard`,
`parental level of education_associate's degree`,
`test preparation course_completed`)
student_df = read_csv("StudentsPerformance.csv")
head(student_df)
library(fastDummies)
student_df <- dummy_cols(student_df, remove_first_dummy = TRUE)
student_df <- student_df %>%
select(-`race/ethnicity_group A`,
-`gender_female`,
-`lunch_standard`,
-`parental level of education_associate's degree`,
-`test preparation course_completed`)
student_df = read_csv("StudentsPerformance.csv")
head(student_df)
library(fastDummies)
student_df <- dummy_cols(student_df)
student_df <- student_df %>%
select(-`race/ethnicity_group A`,
-`gender_female`,
-`lunch_standard`,
-`parental level of education_associate's degree`,
-`test preparation course_completed`)
student_df
set.seed(32022)
n=nrow(student_df)
index = sample(1:n,100)
student_df %>%
slice(index) -> studentTrain
student_df %>%
slice(-index) -> studentValid
model1 = glm(`math score` ~ `reading score` + `writing score` ,data = studentTrain)
summary(model1)
R1 = 1 - model1$deviance/model1$null.deviance
print(R1)
model1 = glm(`math score` ~ .,studentTrain)
model2 = glm(`math score` ~ ., data=studentTrain)
summary(model2)
student_df = read_csv("StudentsPerformance.csv")
head(student_df)
student_df[, !sapply(df, is.character)]
student_df <- dummy_cols(
student_df,
remove_first_dummy = TRUE,  # Avoid dummy variable trap
remove_selected_columns = TRUE  # Remove original categorical columns
)
# View the updated dataframe
head(student_df)
library(fastDummies)
student_df <- dummy_cols(
student_df,
remove_selected_columns = TRUE  # Remove original categorical columns
)
student_df = read_csv("StudentsPerformance.csv")
head(student_df)
library(fastDummies)
student_df <- dummy_cols(
student_df,
remove_selected_columns = TRUE  # Remove original categorical columns
)
student_df <- student_df %>%
select(-`race/ethnicity_group A`,
-`gender_female`,
-`lunch_standard`,
-`parental level of education_associate's degree`,
-`test preparation course_completed`)
student_df
set.seed(32022)
n=nrow(student_df)
index = sample(1:n,100)
student_df %>%
slice(index) -> studentTrain
student_df %>%
slice(-index) -> studentValid
model1 = glm(`math score` ~ `reading score` + `writing score` ,data = studentTrain)
summary(model1)
R1 = 1 - model1$deviance/model1$null.deviance
print(R1)
model2 = glm(`math score` ~ ., data=studentTrain)
summary(model2)
R1 = 1 - model2$deviance/model2$null.deviance
print(R1)
pred1 = predict(model1,dataValid)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(dataValid$math_score,pred1)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(forecast)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = fabletools::accuracy()(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = fabletools::accuracy()(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = fabletools::accuracy()(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = fabletools::accuracy()(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = fabletools::accuracy()(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = fabletools::accuracy()(studentValid$math_score,pred1)
library(fpp3)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
pred1 = predict(model1,studentValid)
pred2 = predict(model2,studentValid)
Acc1 = accuracy(studentValid$math_score,pred1)
library(Metrics) # For accuracy metrics like RMSE, MAE
install.packages("Metrics")
library(Metrics) # For accuracy metrics like RMSE, MAE
# Calculate RMSE and MAE for each model
rmse1 <- rmse(studentValid$math_score, pred1)
mae1 <- mae(studentValid$math_score, pred1)
rmse2 <- rmse(studentValid$math_score, pred2)
mae2 <- mae(studentValid$math_score, pred2)
# Combine results into a data frame
Acc <- data.frame(
Model = c("Model1", "Model2"),
RMSE = c(rmse1, rmse2),
MAE = c(mae1, mae2)
)
print(Acc)
View(studentValid)
library(Metrics) # For accuracy metrics like RMSE, MAE
# Calculate RMSE and MAE for each model
rmse1 <- rmse(studentValid$`math score`, pred1)
mae1 <- mae(studentValid$math_score, pred1)
rmse2 <- rmse(studentValid$math_score, pred2)
mae2 <- mae(studentValid$math_score, pred2)
# Combine results into a data frame
Acc <- data.frame(
Model = c("Model1", "Model2"),
RMSE = c(rmse1, rmse2),
MAE = c(mae1, mae2)
)
print(Acc)
library(Metrics) # For accuracy metrics like RMSE, MAE
# Calculate RMSE and MAE for each model
rmse1 <- rmse(studentValid$`math score`, pred1)
mae1 <- mae(studentValid$`math score`, pred1)
rmse2 <- rmse(studentValid$`math score`, pred2)
mae2 <- mae(studentValid$`math score`, pred2)
# Combine results into a data frame
Acc <- data.frame(
Model = c("Model1", "Model2"),
RMSE = c(rmse1, rmse2),
MAE = c(mae1, mae2)
)
print(Acc)
calc_all_metrics <- function(actual, predicted) {
ME <- mean(actual - predicted)  # Mean Error
RMSE <- rmse(actual, predicted)  # Root Mean Squared Error
MAE <- mae(actual, predicted)  # Mean Absolute Error
MPE <- mean((actual - predicted) / actual * 100)  # Mean Percentage Error
MAPE <- mape(actual, predicted)  # Mean Absolute Percentage Error
return(data.frame(ME, RMSE, MAE, MPE, MAPE))
}
calc_all_metrics <- function(actual, predicted) {
ME <- mean(actual - predicted)  # Mean Error
RMSE <- rmse(actual, predicted)  # Root Mean Squared Error
MAE <- mae(actual, predicted)  # Mean Absolute Error
MPE <- mean((actual - predicted) / actual * 100)  # Mean Percentage Error
MAPE <- mape(actual, predicted)  # Mean Absolute Percentage Error
return(data.frame(ME, RMSE, MAE, MPE, MAPE))
}
Acc1 <- calc_all_metrics(studentValid$`math score`, pred1)
Acc2 <- calc_all_metrics(studentValid$`math score`, pred2)
# Add model names
rownames(Acc1) <- "Model1"
rownames(Acc2) <- "Model2"
# Combine results into a single data frame
Acc <- rbind(Acc1, Acc2)
# Print the accuracy metrics
print(Acc)
rmse1 <- rmse(studentValid$`math score`, pred1)
mae1 <- mae(studentValid$`math score`, pred1)
rmse2 <- rmse(studentValid$`math score`, pred2)
mae2 <- mae(studentValid$`math score`, pred2)
Acc <- data.frame(
Model = c("Model1", "Model2"),
RMSE = c(rmse1, rmse2),
MAE = c(mae1, mae2)
)
print(Acc)
library(tidyverse)
library(glmnet)
library(fastDummies)
library(Metrics)
R1 = 1 - model2$deviance/model2$null.deviance
R1adj = 1-((100-1)/(100-15-1))*(1-R1)
print(R1)
print(R1adj)
R1 = 1 - model2$deviance/model2$null.deviance
R1adj = 1-((100-1)/(100-15-1))*(1-R1)
message(sprintf("R-square", R1))
print(R1)
print(R1adj)
R1 = 1 - model2$deviance/model2$null.deviance
R1adj = 1-((100-1)/(100-15-1))*(1-R1)
message(sprintf("R-square: %.4f", R1))
message(sprintf("Adjusted R-square: %.4f", R1adj))
ridgeregcv = cv.glmnet(x = as.matrix(studentTrain%>%select(-`math score`)),
y = studentTrain$`math score`,
alpha=0)
#From which we obtain the best lambda value
bestlam = ridgeregcv$lambda.min
print(bestlam)
ridgereg = cv.glmnet(x = as.matrix(studentTrain%>%select(-`math score`)),
y = studentTrain$`math score`,
alpha=0,
lambda = bestlam)
ridgereg = cv.glmnet(x = as.matrix(studentTrain%>%select(-`math score`)),
y = studentTrain$`math score`,
alpha=0,
lambda = bestlam)
ridgereg = glmnet(x = as.matrix(studentTrain%>%select(-`math score`)),
y = studentTrain$`math score`,
alpha=0,
lambda = bestlam)
# Which provides a linear model with the coefficients:
print(round(coef(ridgereg),4))
Mreg = round(coef(model2),4)
RidgeReg = as.numeric(round(coef(ridgereg),4))
rbind(Mreg,RidgeReg)
lassoregcv = cv.glmnet(x = as.matrix(studentTrain%>%select(-`math score`)),
y = studentTrain$`math score`,
alpha=1)
#From which we obtain the best lambda value
oneselambda = lassoregcv$lambda.1se
print(oneselambda)
plot(lassoregcv)
lassoreg = glmnet(x = as.matrix(studentTrain%>%select(-`math score`)),
y = studentTrain$`math score`,
alpha=1,
lambda = lassoreg)
lassoreg = glmnet(x = as.matrix(studentTrain%>%select(-`math score`)),
y = studentTrain$`math score`,
alpha=1,
lambda = oneselambda)
# Which provides a linear model with the coefficients:
print(round(coef(lassoreg),4))
lambda = seq(0.1,7.5,0.1)
# So that we get:
lassoPath = glmnet(x = studentTrain%>%select(-`math score`),
y = studentTrain$`math score`,
alpha = 1,lambda = lambda)
Coeffmat <-as.data.frame(as.matrix(t(coef(lassoPath))))%>%
add_column(lambda=lambda[seq(length(lambda),1,-1)])
View(Coeffmat)
Coeffmat.columns
head(Coeffmat)
Coeffmat %>%
pivot_longer(cols = -c(lambda, Intercept), names_to = 'key', values_to = 'Coefficients') %>%
ggplot(aes(x = log(lambda), y = Coefficients, color = key)) +
geom_line() +
facet_wrap(vars(key), scales = 'free_y')
Coeffmat %>%
pivot_longer(cols = -c(lambda, `(Intercept)`, names_to = 'key', values_to = 'Coefficients') %>%
ggplot(aes(x = log(lambda), y = Coefficients, color = key)) +
geom_line() +
facet_wrap(vars(key), scales = 'free_y')
Coeffmat %>%
pivot_longer(cols = -c(lambda, `(Intercept)`), names_to = 'key', values_to = 'Coefficients') %>%
ggplot(aes(x = log(lambda), y = Coefficients, color = key)) +
geom_line() +
facet_wrap(vars(key), scales = 'free_y')
elasnetcv = cv.glmnet(x = as.matrix(studentTrain%>%select(-`math score`)),
y = studentTrain$`math score`,
alpha=0.5)
#From which we obtain the best lambda value
bestlamelasnet = elasnetcv$lambda.min
print(bestlamelasnet)
print(coef(elasnetcv))
predRidge = predict(ridgereg,newx = as.matrix(studentValid%>%select(rownames(ridgereg$beta))))
predLasso = predict(lassoreg,newx = as.matrix(studentValid%>%select(rownames(lassoreg$beta))))
# For the elastic net we must first create the model:
elastnet = glmnet(x = studentTrain%>%select(-`math score`),
y = studentTrain$`math score`,
alpha = 0.5,
lambda = bestlamelasnet)
predElasNet = predict(elastnet,newx = as.matrix(studentValid%>%select(-`math score`)))
# Then we evaluate predictive accuracy as:
Acc3 = accuracy(studentValid$`math score`,predRidge)
rownames(Acc3) = 'Ridge'
View(predRidge)
predRidge = predict(ridgereg,newx = as.matrix(studentValid%>%select(rownames(ridgereg$beta))))
predLasso = predict(lassoreg,newx = as.matrix(studentValid%>%select(rownames(lassoreg$beta))))
# For the elastic net we must first create the model:
elastnet = glmnet(x = studentTrain%>%select(-`math score`),
y = studentTrain$`math score`,
alpha = 0.5,
lambda = bestlamelasnet)
predElasNet = predict(elastnet,newx = as.matrix(studentValid%>%select(-`math score`)))
# Then we evaluate predictive accuracy as:
Acc3 = accuracy(studentValid$`math score`,predRidge)
rownames(Acc3) = 'Ridge'
predRidge = predict(ridgereg,newx = as.matrix(studentValid%>%select(rownames(ridgereg$beta))))
predLasso = predict(lassoreg,newx = as.matrix(studentValid%>%select(rownames(lassoreg$beta))))
# For the elastic net we must first create the model:
elastnet = glmnet(x = studentTrain%>%select(-`math score`),
y = studentTrain$`math score`,
alpha = 0.5,
lambda = bestlamelasnet)
predElasNet = predict(elastnet,newx = as.matrix(studentValid%>%select(-`math score`)))
# Then we evaluate predictive accuracy as:
# Calculate predictive accuracy
Acc3 = as.data.frame(t(accuracy(studentValid$`math score`, predRidge)))
rownames(Acc3) = 'Ridge'
Acc4 = as.data.frame(t(accuracy(studentValid$`math score`, predLasso)))
rownames(Acc4) = 'Lasso'
Acc5 = as.data.frame(t(accuracy(studentValid$`math score`, predElasNet)))
rownames(Acc5) = 'ElasNet'
# Combine results
results = rbind(Acc3, Acc4, Acc5)
print(results)
predRidge = predict(ridgereg,newx = as.matrix(studentValid%>%select(rownames(ridgereg$beta))))
predLasso = predict(lassoreg,newx = as.matrix(studentValid%>%select(rownames(lassoreg$beta))))
# For the elastic net we must first create the model:
elastnet = glmnet(x = studentTrain%>%select(-`math score`),
y = studentTrain$`math score`,
alpha = 0.5,
lambda = bestlamelasnet)
predElasNet = predict(elastnet,newx = as.matrix(studentValid%>%select(-`math score`)))
